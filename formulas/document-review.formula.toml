# Document Review Convoy Formula
#
# Parallel multi-lens document review. Spawns reviewers in parallel,
# each applying a different review lens per /document-review.
# Results synthesized into a unified review with gate assessment.
#
# Standalone usage:
#   gt formula run document-review --topic="Notification redesign" --document="docs/designs/ks-abc-notification.md"
#
# Composed into a workflow via compose.expand:
#   [[compose.expand]]
#   target = "dispatch-reviews"
#   with = "document-review"

description = """
Parallel multi-lens document review convoy. Each leg applies /document-review
with a different review lens. Findings are synthesized into a unified review with
BLOCK/CONCERN/NOTE classification and a binary READY/NOT READY gate assessment.

## Legs (parallel execution)
- **feasibility**: Technical viability and implementation risk
- **adversarial**: Red-team / devil's advocate challenge
- **completeness**: Coverage gaps and missing requirements
- **consistency**: Internal coherence and contradictions
- **risk**: Failure modes and mitigation gaps
- **assumptions**: Hidden dependencies and load-bearing assumptions
- **user-impact**: End-user perspective and migration concerns
- **cost**: Complexity budget and simpler alternatives

## Domain-Specific Lenses (deep expertise)
- **security-audit**: Threat modeling, OWASP, trust boundaries
- **performance**: Latency budgets, throughput, scaling
- **backward-compat**: API contracts, data formats, migration

## Presets
- **minimum**: Core lenses for design advancement (feasibility, adversarial, completeness)
- **full**: All 11 lenses for thorough review
- **risk-focused**: Risk and feasibility emphasis (feasibility, adversarial, risk, assumptions)
- **security**: Security-focused (security-audit, adversarial, risk, feasibility)
- **api-change**: API and compatibility focus (backward-compat, user-impact, feasibility, completeness)
- **scaling**: Performance and scaling focus (performance, feasibility, risk, cost)
- **multi-model-minimum**: Core lenses with cross-model diversity (Claude, Gemini, Convex)
- **multi-model-full**: All lenses distributed across three model backends

## Multi-Model Adversarial Review
Legs can specify an `agent` field to override the model backend. Different models
catch different issue classes — cross-model disagreements are high-signal findings.
The dispatcher uses `gt sling <bead> <rig> --agent <agent>` per leg.

Available agents: claude, gemini, convex (or custom aliases defined in rig config).

When multi-model presets are used, the synthesis step includes a Cross-Model
Disagreement Analysis section that highlights where models diverge.

## Execution Model
1. Each leg spawns as a separate polecat with an assigned lens
2. Polecats work in parallel per /document-review
3. If a leg specifies `agent`, the dispatcher uses `--agent` to select the model
4. Each commits their review to the branch
5. Synthesis step combines all findings into unified gate assessment
6. When multiple models are used, synthesis surfaces cross-model disagreements
"""
formula = "document-review"
# NOTE: intended type is "convoy" (parallel dispatch) — using "workflow" until
# bd adds convoy as a valid formula type. Structure uses [[legs]] not [[steps]].
type = "workflow"
version = 1

[inputs]
[inputs.topic]
description = "Human-readable description of what's being reviewed"
type = "string"
required = true

[inputs.document]
description = "Path to the document under review (relative to branch root)"
type = "string"
required = true

[inputs.parent_bead]
description = "Parent bead ID for creating review child beads"
type = "string"
required = true

[prompts]
base = """
# Document Review Assignment

You are a specialized document reviewer participating in a review convoy.

## Context
- **Topic**: {{.topic}}
- **Document**: {{.document}}
- **Your lens**: {{.leg.title}}
- **Leg ID**: {{.leg.id}}
- **Model**: {{.leg.agent | default "default"}}

## Your Task
Review the document through the **{{.leg.focus}}** lens per **/document-review**.

{{.leg.description}}

## Output Requirements
Write your review to: **{{.output_path}}**

Follow the output format defined in /document-review:

```
## Review: {{.topic}}
Lens: {{.leg.focus}}
Reviewer: {{.leg.id}}
Model: {{.leg.agent | default "default"}}

### Gate Assessment
[For each lens key question, provide a binary verdict:]
[Question]: READY/NOT READY

Overall: READY / NOT READY

### BLOCK
- [Section]: [Finding. Why it blocks. Suggested resolution.]

### CONCERN
- [Section]: [Finding. Why it matters. What would resolve it.]

### NOTE
- [Section]: [Observation. Optional suggestion.]

### Coverage
Sections reviewed: [list]
Lens questions applied: [list]
```

Read the FULL document before evaluating. Do not start writing findings after
the first section. Classify every finding as BLOCK, CONCERN, or NOTE.
"""

[output]
directory = "docs/reviews/{{.review_id}}"
leg_pattern = "{{.leg.id}}-review.md"
synthesis = "review-synthesis.md"

# ============================================================================
# CORE LENSES — included in minimum preset
# ============================================================================

[[legs]]
id = "feasibility"
title = "Feasibility Review"
focus = "Technical viability and implementation risk"
# agent = "claude"  # Optional: override model backend (used by multi-model presets)
description = """
Review the document through a feasibility lens per /document-review.

**Key questions:**
- Can this actually be built with available resources and constraints?
- What's the hardest part? What could go wrong during implementation?
- Are the effort estimates realistic? What's being underestimated?
- Are there technical dependencies that haven't been validated?

**Look for:**
- Proposals that assume capabilities not yet proven
- Optimistic timelines missing integration complexity
- Unstated technical prerequisites
- Architectural decisions that constrain future options
"""

[[legs]]
id = "adversarial"
title = "Adversarial Review"
focus = "Red-team / devil's advocate challenge"
# agent = "gemini"  # Optional: override model backend (used by multi-model presets)
description = """
Review the document through an adversarial lens per /document-review.

**Key questions:**
- What's the strongest argument against this approach?
- What would a skeptic challenge? Where is the reasoning weakest?
- What failure modes are not discussed?
- If this design is wrong, how would we know?

**Look for:**
- Confirmation bias in the analysis (only supporting evidence cited)
- Dismissed alternatives that deserve more consideration
- Assumptions presented as facts
- Missing "what if we're wrong about X?" analysis
"""

[[legs]]
id = "completeness"
title = "Completeness Review"
focus = "Coverage gaps and missing requirements"
# agent = "convex"  # Optional: override model backend (used by multi-model presets)
description = """
Review the document through a completeness lens per /document-review.

**Key questions:**
- Does it address every stated requirement? What's missing?
- Are edge cases considered? What about error paths?
- Is the scope clearly bounded? What's explicitly out of scope?
- Are success criteria defined and measurable?

**Look for:**
- Requirements mentioned but not addressed in the design
- Missing sections expected for this document type
- Undefined behavior in edge cases
- Vague deliverables without acceptance criteria
"""

# ============================================================================
# EXTENDED LENSES — included in full preset
# ============================================================================

[[legs]]
id = "consistency"
title = "Consistency Review"
focus = "Internal coherence and contradictions"
description = """
Review the document through a consistency lens per /document-review.

**Key questions:**
- Do sections contradict each other?
- Do assumptions in one section hold in another?
- Does terminology stay consistent throughout?
- Do the options analysis and recommendation align?

**Look for:**
- Section 3 assuming X while Section 5 assumes not-X
- Terminology drift (same concept, different names)
- Recommendation that doesn't follow from the analysis
- Constraints stated early but violated later
"""

[[legs]]
id = "risk"
title = "Risk Review"
focus = "Failure modes and mitigation gaps"
description = """
Review the document through a risk lens per /document-review.

**Key questions:**
- What are the biggest risks? Are they identified?
- What happens if key assumptions are wrong?
- What's the blast radius of failure?
- Are mitigations proportional to the risks?

**Look for:**
- Unidentified single points of failure
- Missing rollback or recovery plans
- Risks acknowledged but not mitigated
- Cascading failure scenarios not considered
"""

[[legs]]
id = "assumptions"
title = "Assumptions Review"
focus = "Hidden dependencies and load-bearing assumptions"
description = """
Review the document through an assumptions lens per /document-review.

**Key questions:**
- What's taken for granted that should be stated explicitly?
- Which assumptions are load-bearing (design fails if wrong)?
- Have load-bearing assumptions been validated?
- What breaks if external dependencies change?

**Look for:**
- Implicit assumptions buried in prose rather than called out
- "Obviously" or "clearly" signaling unexamined assumptions
- Dependencies on external systems/APIs/behaviors not under our control
- Assumptions that were true historically but may not hold
"""

[[legs]]
id = "user-impact"
title = "User Impact Review"
focus = "End-user perspective and migration concerns"
description = """
Review the document through a user-impact lens per /document-review.

**Key questions:**
- How does this affect end users? Is the impact assessed?
- What's the migration story? Is there a transition plan?
- Are there UX implications not addressed?
- Does this create new failure modes visible to users?

**Look for:**
- Changes that affect user workflows without migration guidance
- Breaking changes presented as non-breaking
- Missing user-facing documentation or communication plan
- Performance regressions visible to users
"""

[[legs]]
id = "cost"
title = "Cost Review"
focus = "Complexity budget and simpler alternatives"
description = """
Review the document through a cost lens per /document-review.

**Key questions:**
- Is this the simplest approach that works?
- What's the ongoing maintenance burden?
- Could a simpler alternative achieve 80% of the value?
- What's the total cost of ownership (build + maintain + operate)?

**Look for:**
- Over-engineering for hypothetical future requirements
- Complex solutions where simple ones would suffice
- Hidden operational costs (monitoring, on-call, upgrades)
- Premature abstraction or generalization
"""

# ============================================================================
# DOMAIN-SPECIFIC LENSES — deep expertise for specialized reviews
# ============================================================================

[[legs]]
id = "security-audit"
title = "Security Audit Review"
focus = "Threat modeling and vulnerability surface"
description = """
Review the document through a security-audit lens per /document-review.

**Key questions:**
- Does the design introduce new attack surface? Where are trust boundaries?
- Are inputs validated at system boundaries (user input, external APIs)?
- Does it follow OWASP Top 10 mitigations for the relevant categories?
- Is there an authentication/authorization model? Is it least-privilege?
- What's the threat model? Who are the adversaries and what can they do?
- Are secrets, tokens, and credentials handled securely (no hardcoding, proper rotation)?

**Look for:**
- Missing input validation or output encoding at trust boundaries
- Authentication/authorization bypasses or confused-deputy risks
- Sensitive data in logs, URLs, error messages, or client-side storage
- Missing rate limiting, CSRF protection, or injection prevention
- Cryptographic choices that are non-standard or rolled-your-own
- Implicit trust assumptions ("internal network is safe", "users won't do X")
"""

[[legs]]
id = "performance"
title = "Performance Review"
focus = "Latency budgets, throughput, and scaling"
description = """
Review the document through a performance lens per /document-review.

**Key questions:**
- Are latency budgets defined for critical user-facing paths?
- What's the expected load profile and how does the design scale with it?
- Are there hot paths, tight loops, or unbounded operations?
- What happens at 10x current traffic? At 100x?
- Are resource limits (memory, CPU, connections, file descriptors) explicitly bounded?
- Is there a capacity plan linking resource requirements to load?

**Look for:**
- N+1 query patterns or unbounded fan-out in data access
- Missing pagination, caching strategy, or connection pooling
- Synchronous operations on critical paths that could be async
- Missing backpressure or queue depth limits
- Latency-sensitive paths that cross network boundaries unnecessarily
- Optimistic capacity assumptions without load testing plans
"""

[[legs]]
id = "backward-compat"
title = "Backward Compatibility Review"
focus = "API contracts, data formats, and migration"
description = """
Review the document through a backward-compat lens per /document-review.

**Key questions:**
- Does this break existing API contracts (REST, gRPC, CLI, SDK)?
- Are wire formats and data schemas backward-compatible (additive only)?
- Is there a migration path for existing consumers? Is it documented?
- Are deprecation timelines defined with sufficient notice periods?
- Can old and new versions coexist during rollout (blue-green, canary)?
- What happens to in-flight requests during deployment?

**Look for:**
- Removed or renamed fields/endpoints without deprecation period
- Changed semantics of existing fields (silent breaking changes)
- Missing version negotiation or feature detection
- Database migrations that can't roll back
- Assumptions that all consumers upgrade simultaneously
- Missing compatibility testing strategy (contract tests, integration tests)
"""

# ============================================================================
# PRESETS — configurable leg selection
# ============================================================================

[presets]
[presets.minimum]
description = "Core lenses for design advancement — fast, focused on blockers"
legs = ["feasibility", "adversarial", "completeness"]

[presets.full]
description = "All available review lenses — thorough review for major designs"
legs = ["feasibility", "adversarial", "completeness", "consistency", "risk", "assumptions", "user-impact", "cost", "security-audit", "performance", "backward-compat"]

[presets.risk-focused]
description = "Risk and feasibility emphasis — for high-stakes or uncertain designs"
legs = ["feasibility", "adversarial", "risk", "assumptions"]

[presets.security]
description = "Security-focused review — for designs touching auth, data handling, or trust boundaries"
legs = ["security-audit", "adversarial", "risk", "feasibility"]

[presets.api-change]
description = "API and compatibility focus — for designs changing public interfaces or data formats"
legs = ["backward-compat", "user-impact", "feasibility", "completeness"]

[presets.scaling]
description = "Performance and scaling focus — for designs with significant load or latency requirements"
legs = ["performance", "feasibility", "risk", "cost"]

[presets.multi-model-minimum]
description = "Core lenses with cross-model diversity — each lens uses a different model backend"
legs = ["feasibility", "adversarial", "completeness"]
[presets.multi-model-minimum.agents]
feasibility = "claude"
adversarial = "gemini"
completeness = "convex"

[presets.multi-model-full]
description = "All lenses distributed across three model backends for maximum diversity"
legs = ["feasibility", "adversarial", "completeness", "consistency", "risk", "assumptions", "user-impact", "cost"]
[presets.multi-model-full.agents]
feasibility = "claude"
adversarial = "gemini"
completeness = "convex"
consistency = "claude"
risk = "gemini"
assumptions = "convex"
user-impact = "claude"
cost = "gemini"

# ============================================================================
# SYNTHESIS — combines all leg outputs
# ============================================================================

[synthesis]
title = "Review Synthesis"
description = """
Combine all lens reviews into a unified review document.

**Your input:**
All lens reviews from: {{.output.directory}}/

**Your output:**
A synthesized review at: {{.output.directory}}/{{.output.synthesis}}

**Structure:**
```markdown
# Review Synthesis: {{.topic}}

## Gate Assessment
[Aggregate READY/NOT READY across all lenses]
Overall: READY / NOT READY

## Cross-Model Disagreement Analysis
[INCLUDE THIS SECTION ONLY when multiple model backends were used]

High-signal findings where different models diverged:

### Gate Divergence
[Cases where one model says READY but another says NOT READY on the same
criterion or section. List each divergence with both models' reasoning.]

### Severity Disagreement
[Cases where models classify the same finding at different severity levels
(e.g., one model flags BLOCK, another flags CONCERN or NOTE). Include both
assessments — the disagreement itself is the signal.]

### Unique Findings
[Findings surfaced by only one model that others missed entirely. Note which
model found it and which models did not. These are blind-spot indicators.]

### Interpretation
[What do the disagreements tell us? Are they about different risk tolerance,
different domain expertise, or genuinely ambiguous areas? Disagreements on
the same evidence suggest the question needs human judgment.]

## Consensus
What all reviewers agree on. Areas of convergent concern or approval.

## Conflicts
Where reviewers disagree, with both sides stated. Which lens found what.

## Blockers
All BLOCK findings across lenses, deduplicated. Each with:
- Source lens (and model, if multi-model)
- Section reference
- Finding and suggested resolution

## Concerns
All CONCERN findings, grouped by theme. Note which lenses surfaced each.

## Open Questions
Questions requiring human judgment. Batched for the human gate —
do not present them piecemeal.

## Revised Recommendation
If the document contains a recommendation, state whether the review
findings support it, challenge it, or require modifications. Be specific
about what should change.

## Appendix: Per-Lens Summaries
[Brief summary of each lens's findings with link to full review]
[Include model backend for each lens when multi-model preset was used]
```

Deduplicate findings across lenses (note which lenses found them).
Prioritize by severity (BLOCK > CONCERN > NOTE).
Batch all questions for the human gate.

**Cross-model disagreement priority:** When multiple models are used,
disagreements between models on the SAME finding are higher priority than
agreement. Agreement confirms the finding; disagreement indicates areas
where human judgment is needed. Surface disagreements prominently.
"""
depends_on = ["feasibility", "adversarial", "completeness", "consistency", "risk", "assumptions", "user-impact", "cost", "security-audit", "performance", "backward-compat"]
